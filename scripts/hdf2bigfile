#! python

import bigfile
import logging
import argparse
import os

ap = argparse.ArgumentParser('hdf2bigfile',
    description="""
Converting a HDF5 file to a bigfile.

DataGroup is converted as sub-file,
Composite Data Types are converted as sub-file,
Simple Columns are converted as BigBlock.

Shape are converted to ndarray.shape attribute.

Vector columns in a DataSet is converted to 2d tables (nmemb >= 1)
Multidimensional arrays are flattened, and the orginal shape is
saved as ndarray.shape attribute.

Strides are lost: arrays are always written as C-contiguous.

Currently, attributes are not carried over.
Currently, include and exclude filters do not work.

""")

ap.add_argument("source")
ap.add_argument("dest")
ap.add_argument("--verify", action='store_true', default=True)
ap.add_argument("--no-verify", dest='verify', action='store_false', default=True)
ap.add_argument("--include", action="append")
ap.add_argument("--exclude", action="append")

def ishdf5(path):
    if os.path.exists(path):
        if not os.path.isfile(path):
            return False
    if path.endswith((".hdf5", ".hdf", ".h5")):
        return True
    return False

def isfits(path):
    if os.path.exists(path):
        if not os.path.isfile(path):
            return False
    if path.endswith((".fits", ".fz", ".fits.gz")):
        return True
    return False

def isbigfile(path):
    if os.path.exists(path):
        if not os.path.isdir(path):
            return False
    return True

def main(ns):
    if ishdf5(ns.source):
        reader = HDFReader
    elif isfits(ns.source):
        reader = FITSReader
    elif isbigfile(ns.source):
        reader = BigFileReader

    if ishdf5(ns.dest):
        writer = HDFWriter
        verify = HDFReader if ns.verify else None
    elif isfits(ns.dest):
        writer = FITSWriter
        verify = FITSReader if ns.verify else None
    elif isbigfile(ns.dest):
        writer = BigFileWriter
        verify = BigFileReader if ns.verify else None

    convert(ns.source, ns.dest, reader, writer, verify)

class ReadProxy(object):
    def __init__(self, obj):
        self.obj = obj
    def __getitem__(self, index):
        return self.obj[index].view(dtype=self.dtype)
    def __len__(self):
        return self.shape[0]
    def __repr__(self):
        return repr(self.obj)

class IOBase:
    def __enter__(self):
        self.fileobj.__enter__()
        return self

    def __exit__(self, *args):
        self.fileobj.__exit__(*args)

    def traverse(self, operation, prefix="", dtype=None):
        children = self.getchildren(prefix)
        for k in children:
            path = '/'.join([prefix, k])
            self.traverse(operation, path)

        if len(children) == 0:
            srcobj = self.getobj(prefix)
            # operation(srcobj, prefix)
            try:
                operation(srcobj, prefix)
            except Exception as e:
                print('Error processing %s' % prefix, e)

    def verify(self, srcobj, path):
        print("Verifying %s against %s" % (str(srcobj), path))
        from numpy.testing import assert_allclose
        destobj = self.getobj(path)
        if len(srcobj) > 0:
        #    assert (srcobj.dtype == destobj.dtype)
            assert tuple(srcobj.shape) == tuple(destobj.shape)

        src = srcobj[:].ravel()
        dest = destobj[:].ravel()
        if src.dtype.fields:
            for name in src.dtype.names:
                assert_allclose(src[name], dest[name])
        else:
            assert_allclose(src, dest)

class HDFReader(IOBase):
    def __init__(self, path):
        import h5py
        self.fileobj = h5py.File(path, mode='r')

    def getobj(self, path):
        obj = self.fileobj
        for seg in path.split('/'):
            if len(seg) == 0: continue
            if hasattr(obj, 'keys'): # datagroup
                obj = obj[seg]
            elif hasattr(obj, 'dtype'): # dataset
                obj = obj[seg]
        return obj

    def getchildren(self, prefix):
        srcobj = self.getobj(prefix)
        if hasattr(srcobj, 'keys'):
            # this is a datagroup
            return list(srcobj.keys())
        if hasattr(srcobj, 'dtype'):
            if srcobj.dtype.names is not None:
                return list(srcobj.dtype.names)
            else:
                return [] # leaf
        raise TypeError("Unknown data block type")

class node(dict): pass

def _blocks_to_tree(blocks):
    tree = node()
    node.final = False
    node.path = '/'
    def _add(tree, b):
        obj = tree
        path = ""
        for seg in b.split('/'):
            if len(seg) == 0: continue
            obj = obj.setdefault(seg, node())
            path = path + '/' + seg
            obj.path = path
            obj.final = False
        obj.final = True

    for b in blocks:
       _add(tree, b)

    return tree

class BigFileReader(IOBase):
    def __init__(self, path):
        self.fileobj = bigfile.File(path, create=False)
        self.tree = _blocks_to_tree(self.fileobj.blocks)

    def getnode(self, path):
        obj = self.tree
        for seg in path.split('/'):
            if len(seg) == 0: continue
            obj = obj[seg]
        return obj

    def _getobj_from_node(self, node):
        if node.final:
            obj = self.fileobj[node.path]
        else:
            obj = self.fileobj[node.path + '/']
            try:
                obj = bigfile.Dataset(obj)
                return obj
            except Exception as e:
                pass

        return obj

    def getobj(self, path):
        node = self.getnode(path)
        obj = self._getobj_from_node(node)

        obj1 = ReadProxy(obj)
        obj1.dtype = obj.dtype
        obj1.size = obj.size
        if hasattr(obj, 'attrs') and 'ndarray.shape' in obj.attrs:
            # terminal
            obj1.shape = obj.attrs['ndarray.shape']
        else:
            if hasattr(obj, 'shape'):
                obj1.shape = obj.shape
            else:
                obj1.shape = (obj.size, )
        if hasattr(obj, 'attrs'):
            obj1.attrs = dict(obj.attrs)
        else:
            obj1.attrs = None

        obj = obj1
        return obj

    def getchildren(self, prefix):
        node = self.getnode(prefix)
        obj = self._getobj_from_node(node)
        if isinstance(obj, (bigfile.Dataset, bigfile.Column)):
            return [] # leaf
        else:
            # this is a datagroup
            return list(node.keys())
        raise TypeError("Unknown data block type")

class BigFileWriter(IOBase):
    def __init__(self, path):
        self.fileobj = bigfile.File(path, create=True)

    def create(self, srcobj, path):
        dtype = srcobj.dtype

        if len(dtype.shape) > 1:
            raise ValueError("vector types unsupported")

        isvector = len(dtype.shape) != 0

        if not isvector:
            size = srcobj.size
        else:
            size, Nmemb = srcobj.shape
            assert Nmemb == dtype.shape[0]

        bb = self.fileobj.create(path, dtype, size=size)
        bb.write(0, srcobj[:].ravel())

        if not isvector:
            bb.attrs['ndarray.shape'] = srcobj.shape

class HDFWriter(IOBase):
    def __init__(self, path):
        import h5py
        self.fileobj = h5py.File(path, mode='w')

    def create(self, srcobj, path):
        dtype = srcobj.dtype

        obj = self.fileobj
        segs = [seg for seg in path.split('/') if len(seg) > 0]
        final = segs[-1]

        leading = '/'.join(segs[:-1])
        if len(leading) == 0:
            leading = '/'
        grp = obj.require_group(leading)
        bb = grp.create_dataset(final, data=srcobj[:])

        if srcobj.attrs is not None:
            for key, value in srcobj.attrs.items():
                try:
                    bb.attrs[key] = value
                except Exception as e:
                    print("missed property: %s %s for %s" % (key, value, e))

def _purify_path(path):
    segs = [seg for seg in path.split('/') if len(seg) > 0]
    return '/'.join(segs)
def _basename(path):
    segs = [seg for seg in path.split('/') if len(seg) > 0]
    return segs[-1]

class FITSReader(IOBase):
    def __init__(self, path):
        import fitsio
        self.fileobj = fitsio.FITS(path, mode='r')
        blocks = []
        for i in range(len(self.fileobj)):
            blocks.append(self.fileobj[i].get_extname())
        self.tree = _blocks_to_tree(blocks)

    def getnode(self, path):
        obj = self.tree
        for seg in path.split('/'):
            if len(seg) == 0: continue
            obj = obj[seg]
        return obj

    def _getobj_from_node(self, node):
        obj = self.fileobj[node.path.lstrip('/')]
        return obj

    def getobj(self, path):
        import numpy
        node = self.getnode(path)
        obj = self._getobj_from_node(node)

        obj1 = ReadProxy(obj)
        tbldtype = obj.get_rec_dtype()[0]
        if len(tbldtype.fields) == 1 and tbldtype.names[0] == '_':
            obj1.dtype = tbldtype['_']
        else:
            obj1.dtype = tbldtype

        obj1.shape = (obj.get_nrows(), )
        obj1.size = numpy.prod(obj1.shape, dtype='intp')

        obj1.attrs = dict(obj.read_header())

        obj = obj1
        return obj

    def getchildren(self, prefix):
        node = self.getnode(prefix)

        if node.final:
            return [] # leaf
        else:
            # this is a datagroup
            return list(node.keys())
        raise TypeError("Unknown data block type")

class FITSWriter(IOBase):
    def __init__(self, path):
        import fitsio
        self.fileobj = fitsio.FITS(path, mode='rw', clobber=True)

    @staticmethod
    def _make_compatible_dtype(dtype):
        import numpy
        if dtype.fields is None:
            if dtype == numpy.dtype('u8'):
                return numpy.dtype('i8')
            else:
                return dtype
        else:
            descr = []
            for name, subdescr in dtype.fields.items():
                subdtype, offset = subdescr[:2]
                descr.append((name, (FITSWriter._make_compatible_dtype(subdtype), offset)))
            return numpy.dtype(dict(descr))

    def create(self, srcobj, path):
        dtype = srcobj.dtype

        obj = self.fileobj
        extname = _purify_path(path)

        data = srcobj[:]

        dtype = self._make_compatible_dtype(data.dtype)
        data = data.view(dtype)

        if data.dtype.fields is None:
            # workaround FITS not allowing non-structured data
            data = data.view(dtype=[('_', data.dtype)])

        obj.create_table_hdu(extname=extname, dtype=data.dtype, dims=data.shape)
        bb = obj[extname]
        bb.write(data)
        if srcobj.attrs is not None:
            header = dict([(k, str(v)) for k, v in srcobj.attrs.items()])
            bb.write_keys(header)

        #print(extname, data.shape)
        #print('created table', bb,)

def convert(srcpath, outpath, readertype, writertype, verifiertype):

    with readertype(srcpath) as hin:

        with writertype(outpath) as writer:
            hin.traverse(writer.create)

        if verifiertype:
            with verifiertype(outpath) as reader:
                hin.traverse(reader.verify)


if __name__ == "__main__" :
    ns = ap.parse_args()

    main(ns)
